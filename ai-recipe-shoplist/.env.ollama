# Ollama Configuration
# Install Ollama locally: https://ollama.ai/
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_TIMEOUT=120

# Alternative models you can use:
# OLLAMA_MODEL=llama3.2:1b      # Faster, less accurate
# OLLAMA_MODEL=llama3.2:7b      # Better quality, slower
# OLLAMA_MODEL=codellama:7b     # Good for code understanding
# OLLAMA_MODEL=mistral:7b       # Good general purpose model

# Application Settings
DEBUG=true
LOG_LEVEL=info
AI_PROVIDER=ollama

# Cache Settings
CACHE_TTL=3600
CACHE_ENABLED=true

# Store API Keys (for real integrations)
COLES_API_KEY=your_coles_api_key
WOOLWORTHS_API_KEY=your_woolworths_api_key
ALDI_API_KEY=your_aldi_api_key

# Database (optional)
DATABASE_URL=sqlite:///./recipes.db

# Instructions to set up Ollama:
# 1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh
# 2. Start Ollama: ollama serve
# 3. Pull a model: ollama pull llama3.2:3b
# 4. Test: ollama run llama3.2:3b "Hello, world!"