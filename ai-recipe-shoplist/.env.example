# AI Recipe Shoplist Crawler - Configuration
# =================================================================
# This file contains all configuration for the AI Recipe Shoplist Crawler
# Copy this file to .env and configure the provider you want to use

# =================================================================
# GENERAL APPLICATION SETTINGS
# =================================================================

# AI Provider Selection (choose one: github, openai, azure, ollama, stub)
AI_PROVIDER=github

# Stub AI Responses (for development/testing)
STUB_RESPONSES_PATH=stub_responses   # Path to stub responses directory

# Logging Configuration
LOG_LEVEL=INFO
DEBUG_ENABLED=false

# Web Application Settings
PORT=8000
HOST=0.0.0.0

# Cache Settings (optional)
CACHE_ENABLED=true
CACHE_TTL=3600

# =================================================================
# AI PROVIDER CONFIGURATIONS
# =================================================================
# Configure the provider you selected above

# -----------------------------------------------------------------
# GITHUB MODELS PROVIDER
# -----------------------------------------------------------------
# Free tier with conservative rate limits
# Get your token at: https://github.com/settings/tokens

GITHUB_TOKEN=your_github_token_here
GITHUB_MODEL=gpt-4o-mini
GITHUB_API_URL=https://models.inference.ai.azure.com
GITHUB_MODEL_TIMEOUT=30

# -----------------------------------------------------------------
# OPENAI PROVIDER
# -----------------------------------------------------------------
# Paid service with higher rate limits
# Get your API key at: https://platform.openai.com/api-keys

OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_API_URL=https://api.openai.com/v1
OPENAI_MODEL_TIMEOUT=30

# -----------------------------------------------------------------
# AZURE OPENAI PROVIDER
# -----------------------------------------------------------------
# Enterprise service with highest rate limits
# Configure in Azure Portal: https://portal.azure.com

AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_DEPLOYMENT=gpt-4o-mini
AZURE_MODEL_TIMEOUT=30

# -----------------------------------------------------------------
# OLLAMA PROVIDER (LOCAL)
# -----------------------------------------------------------------
# Local LLM service - no API key required
# Install Ollama: https://ollama.ai

OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_MODEL_TIMEOUT=60

# =================================================================
# RETRY & RATE LIMITING CONFIGURATION
# =================================================================
# Tenacity-based retry configuration for all providers

# -----------------------------------------------------------------
# GLOBAL RETRY DEFAULTS
# -----------------------------------------------------------------
# These serve as fallback values when provider-specific settings are not defined

DEFAULT_MAX_RETRIES=3              # Number of retry attempts for failed requests
DEFAULT_BASE_DELAY=1.0             # Base delay in seconds for exponential backoff
DEFAULT_MAX_DELAY=60.0             # Maximum delay cap in seconds
DEFAULT_RETRY_MULTIPLIER=2.0       # Exponential backoff multiplier (1s → 2s → 4s → 8s...)
DEFAULT_RPM_LIMIT=30               # Default requests per minute limit

# -----------------------------------------------------------------
# GITHUB MODELS RETRY SETTINGS
# -----------------------------------------------------------------
# Conservative settings due to free tier limitations

GITHUB_MAX_RETRIES=3
GITHUB_BASE_DELAY=1.0
GITHUB_MAX_DELAY=60.0
GITHUB_RETRY_MULTIPLIER=2.0
GITHUB_RPM_LIMIT=15               # Conservative limit for free tier

# -----------------------------------------------------------------
# OPENAI RETRY SETTINGS
# -----------------------------------------------------------------
# Moderate settings for paid tier

OPENAI_MAX_RETRIES=3
OPENAI_BASE_DELAY=1.0
OPENAI_MAX_DELAY=60.0
OPENAI_RETRY_MULTIPLIER=2.0
OPENAI_RPM_LIMIT=60               # Higher limits for paid service

# -----------------------------------------------------------------
# AZURE OPENAI RETRY SETTINGS
# -----------------------------------------------------------------
# Higher settings for enterprise tier

AZURE_MAX_RETRIES=3
AZURE_BASE_DELAY=1.0
AZURE_MAX_DELAY=60.0
AZURE_RETRY_MULTIPLIER=2.0
AZURE_RPM_LIMIT=120              # Highest limits for enterprise service

# -----------------------------------------------------------------
# OLLAMA RETRY SETTINGS (LOCAL)
# -----------------------------------------------------------------
# Aggressive settings for local service

OLLAMA_MAX_RETRIES=3
OLLAMA_BASE_DELAY=0.5            # Faster retries for local service
OLLAMA_MAX_DELAY=30.0            # Lower max delay for local service
OLLAMA_RETRY_MULTIPLIER=1.5      # Gentler backoff for local service
OLLAMA_RPM_LIMIT=0               # No rate limiting for local service (0 = unlimited)

# =================================================================
# ADVANCED RETRY SETTINGS
# =================================================================
# Fine-tuning for specific scenarios

# Retry conditions (which HTTP status codes to retry)
RETRY_ON_429=true                # Rate limit exceeded
RETRY_ON_502=true                # Bad gateway
RETRY_ON_503=true                # Service unavailable  
RETRY_ON_504=true                # Gateway timeout
RETRY_ON_TIMEOUT=true            # Network timeouts
RETRY_ON_CONNECTION_ERROR=true   # Connection errors

# Non-retryable errors (never retry these)
NO_RETRY_ON_401=true             # Authentication error
NO_RETRY_ON_403=true             # Forbidden
NO_RETRY_ON_400=true             # Bad request
NO_RETRY_ON_404=true             # Not found
NO_RETRY_ON_422=true             # Unprocessable entity

# Retry logging configuration
RETRY_LOG_LEVEL=WARNING          # Log level for retry attempts (DEBUG, INFO, WARNING, ERROR)
RETRY_LOG_ATTEMPTS=true          # Log each retry attempt
RETRY_LOG_SUCCESS=false          # Log successful attempts after retries
RETRY_LOG_STATISTICS=true        # Log retry statistics

# =================================================================
# HTTP CLIENT CONFIGURATION
# =================================================================
# Advanced performance settings

# Connection pooling for HTTP client
HTTP_POOL_CONNECTIONS=10         # Number of connection pools
HTTP_POOL_MAXSIZE=10            # Maximum connections per pool
HTTP_KEEP_ALIVE_TIMEOUT=30      # Keep-alive timeout in seconds

# Request timeouts
HTTP_CONNECT_TIMEOUT=10.0       # Connection timeout in seconds
HTTP_READ_TIMEOUT=30.0          # Read timeout in seconds
HTTP_WRITE_TIMEOUT=30.0         # Write timeout in seconds

# =================================================================
# QUICK SETUP EXAMPLES
# =================================================================

# Example 1: GitHub Models (Free)
# AI_PROVIDER=github
# GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx

# Example 2: OpenAI (Paid)
# AI_PROVIDER=openai
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx

# Example 3: Azure OpenAI (Enterprise)
# AI_PROVIDER=azure
# AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxx
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Example 4: Ollama (Local)
# AI_PROVIDER=ollama
# OLLAMA_MODEL=llama3.2:3b

# =================================================================
# ENVIRONMENT-SPECIFIC TUNING
# =================================================================

# Production (higher limits and more retries):
# GITHUB_RPM_LIMIT=25
# DEFAULT_MAX_RETRIES=5
# RETRY_LOG_LEVEL=INFO

# Development (more aggressive retries):
# DEFAULT_MAX_RETRIES=5
# DEFAULT_BASE_DELAY=0.5
# DEFAULT_RETRY_MULTIPLIER=1.5

# Conservative/slow networks (longer timeouts):
# HTTP_READ_TIMEOUT=60.0
# DEFAULT_MAX_DELAY=120.0
# DEFAULT_BASE_DELAY=2.0